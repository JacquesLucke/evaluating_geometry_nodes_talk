<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />
    <link rel="icon" type="image/png" href="images/favicon.png" />

    <title>Evaluating Geometry Nodes</title>

    <link rel="stylesheet" href="dist/reset.css" />
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/black.css" />

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/my_theme.css" />

    <script src="https://js.polli.live/main.js"></script>
    <!-- <script src="http://localhost:8080/main.js"></script> -->

    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/d3/7.9.0/d3.min.js"
      integrity="sha512-vc58qvvBdrDR4etbxMdlTt4GBQk1qjvyORR2nrsPsFPyrs+/u5c3+1Ct6upOgdZoIl7eq6k3a1UPDSNAQi/32A=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/plotly.js/2.35.0/plotly.min.js"
      integrity="sha512-L205fVN73b8Ft9dbuwTVGFb4FHVSOPpCLMQzcHa4r0+CuUswxzK/JRW7glZrpC+bO3Yaka0DYYDmtlhVbmnX+g=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <style>
      .reveal h1,
      .reveal h2,
      .reveal h3,
      .reveal h4,
      .reveal h5,
      .reveal h6 {
        text-transform: none;
      }

      hr {
        border-color: #444;
      }

      .reveal em {
        color: #dba667;
        font-style: normal;
      }

      .reveal pre code {
        max-height: 500px;
      }

      .reveal .progress {
        color: #00d6a3ff;
      }

      .fragment.blur {
        filter: blur(0.2em);
      }

      .fragment.blur.visible {
        filter: none;
      }

      .filter-lazy-function-graph-svg {
        filter: invert(90%);
      }

      .combinatorial-explosion-table td:nth-child(1) {
        text-align: center !important;
      }

      .devirtualization-table td:nth-child(2),
      .devirtualization-table td:nth-child(3),
      .devirtualization-table td:nth-child(4) {
        text-align: right !important;
      }
    </style>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2 class="r-fit-text">Evaluating Geometry Nodes</h2>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
          </aside>
        </section>
        <section>
          <div
            style="display: grid; grid-template-columns: 2fr 3fr; height: 100%"
          >
            <div>
              <img
                class="polli-live-qr-code"
                style="
                  image-rendering: pixelated;
                  margin: 0;
                  width: 100%;
                  background-color: white;
                  border: 0.5em solid transparent;
                  border-radius: 0.5em;
                  box-sizing: border-box;
                "
              />
            </div>
            <div>
              Join the interactive part!
              <div
                class="polli-live-choice"
                id="join-interactivity"
                style="width: 100%"
              >
                <option>I'm in</option>
                <option>Wait</option>
              </div>
              <div>
                <em><code>polli.live</code></em> with
                <code class="polli-live-session-id"></code>
              </div>
            </div>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
          </aside>
        </section>
        <section>
          <img src="images/topics_no_fonts.svg" style="width: 70%" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Technical talk for curious and node system devs
            * Simple evaluation approaches, used a lot anyway, to get started
            * Lazy Graph evaluation in geometry nodes
            * Evaluating math functions on large amounts of data
          </aside>
        </section>
        <section>
          <h2>What's your background?</h2>
          <div class="polli-live-continuum" id="what-is-your-background-poll">
            <option>Artistic</option>
            <option>Both</option>
            <option>Technical</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Before that, want to know who is here.
            * "both" is impressive, can do crazy stuff, Simon, bridge gaps
            * I: lead developer, 10 years, hired in 2018, remote with Hans
            * Love seeing shared experiments
          </aside>
        </section>
        <section>
          <h2>Basics</h2>
          <img src="images/get_started.png" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * start simple, general approaches
            * assume people know what this does
          </aside>
        </section>
        <section>
          <h2>Toposort</h2>
          <img src="images/toposort_no_fonts.svg" />
          <hr style="margin-top: 0" />
          <h4>Are there multiple possible orderings?</h4>
          <div class="polli-live-choice" id="unique-toposort" data-hide-answer>
            <option>Yes</option>
            <option>No</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Simplest solution: toposort, dependencies come first
            * cyclic links
            * evaluate in sorted order
            * Animation Nodes + Python script
            * Limitations: unneeded nodes, hard parallelization
            * good when all nodes are active
            * Important concept for other algorithms
            * multiple orderings: yes
          </aside>
        </section>
        <section>
          <h2>Two-Pass Evaluation</h2>
          <img
            src="images/two_pass.png"
            style="margin-top: 60px; margin-bottom: -20px"
          />
          <img src="images/two_pass_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * two passes: filter, evaluate
            * Example, second add node not neeeded, no output used
            * More complex cases:
              * dependency graph: animated visibility
              * cycles: mix node factor 0 or 1
          </aside>
        </section>
        <section>
          <h2>Request Based Evaluation</h2>
          <img src="images/request_based_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * before: find first, then evaluate
            * before: used in shader (eevee, cycles), compositor
            * bad for geometry nodes:
              * can't determine active nodes without evaluation
              * switch nodes depend on each other
              * trust disabled nodes (timing makes explicit)
            * solution: request based evaluation
              * no preprocessing
              * start at output
              * request inputs recursively (see slide)
            * recursive evaluator:
              * was initial implementation
              * limitations: single threaded, easy stack overflow
              * next step: solve more generically (complex with multi-threading)
          </aside>
        </section>
        <section>
          <h2>Lazy Function</h2>
          <pre
            class="cpp"
          ><code data-trim data-line-numbers="2|3|4,5,11|6,7,12,13|8,14"><script type="text/template">
          // Simple lazy switch node.
          bool *condition = p.get_or_request("Condition");
          if (!condition) return;
          if (*condition) {
            p.set_input_unused("False");
            Geometry *value = p.get_or_request("True");
            if (!value) return;
            p.set_output("Value", *value);
          }
          else {
            p.set_input_unused("True");
            Geometry *value = p.get_or_request("False");
            if (!value) return;
            p.set_output("Value", *value);
          }
          </script></code></pre>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Core: lazy function class
              * lazily computes outputs and requests inputs
            * go over example
            * all geometry nodes are lazy functions
            * higher level api for most nodes, without lazyness
            * special nodes use LazyFunction directly
            * independent of geometry nodes (for testing, mental model)
          </aside>
        </section>
        <section>
          <h2>Lazy Function Graph</h2>
          <img
            src="images/simple_lazy_function_graph.png"
            style="width: 80%; margin-bottom: 0px"
          />
          <img
            src="images/simple_lazy_function_graph.svg"
            style="width: 90%; margin-top: 0"
          />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * combine lazy functions to graph
            * similar to node graph, small differences (lifetimes, zones)
            * most complex part: evaluate graph, with message passing
            * modifier, tools: provide inputs and handle outputs
            * lazyness only within modifier
            * Core Debug Tools add-on, extensions platform
          </aside>
        </section>
        <section>
          <h2>Scheduling &#8211; Message Priority</h2>
          <img src="images/shared_mesh_references_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * message types: request, response, unused
            * order matters, see slide
            * mesh copy cheaper nowdays: implicit sharing (explain)
          </aside>
        </section>
        <section>
          <h3>Scheduling &#8211; Breadth or Depth First?</h3>
          <img src="images/breadth_depth_no_fonts.svg" />
          <div
            class="polli-live-choice"
            id="breadth-vs-depth-first"
            data-hide-answer
          >
            <option>Breadth First</option>
            <option>Depth First</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * multiple scheduled nodes: evaluation order?
            * two main strategies:
              * breadth: FIFO, primitives -> bounding -> join
              * depth: LIFO (last in first out), (primitive -> bounding) * 2 -> join
            * analogy:
              * breadth: start many side projects, working memory overload
              * depth: finish first
            * solution: depth first
              * lower peak memory, better cache usage
            * initial breadth for multi-threading, usually comes by itself
          </aside>
        </section>
        <section>
          <h2>Lazy Function Composition</h2>
          <img src="images/lazy_function_composition.svg" />
          <p>
            <code><em>LazyFunction</em></code> &#10230;
            <code><em>Graph</em></code> &#10230;
            <code><em>LazyFunction</em></code>
          </p>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * seen before: can create graph
            * key design aspect: graph itself is lazy function
              * all lazyness propagates
            * build smaller graphs + combine, lossless lazyness
            * used for groups and zones
          </aside>
        </section>
        <section>
          <h2>Node Groups</h2>
          <img src="images/node_group_fade.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * two ways: flatten/inline, separate
            * flatten: preprocess, simpler evaluation, simpler optimizations (two pass evaluation)
            * all other node systems flatten tree
            * Geometry Nodes not:
              * evaluator needs nesting anyway: zones, nesting = flat for evaluator
              * node systems too big (>10.000), often large parts disabled in assets
              * not need for optimizations yet (optional inlining in future?)
          </aside>
        </section>
        <section>
          <h2>Zones</h2>
          <img src="images/repeat_zone.png" style="margin-bottom: -40px" />
          <img src="images/repeat_zone.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * zone becomes node
            * Graph created for zone body, like for groups
            * dynamic graph for repeat/foreach zone, see slide
            * no changes in core evaluator
            * benefit: multiple repeat body evaluations
            * cheap zones: need different solution in future
          </aside>
        </section>
        <section>
          <h2>Scheduling &#8211; Multi Threading</h2>
          <img
            src="images/multi_threading_simple.png"
            style="margin-bottom: 0; width: 80%"
          />
          <hr style="margin-top: -10px; margin-bottom: 10" />
          <h4>How many threads should be used here?</h4>
          <div class="polli-live-choice" id="how-many-threads" data-hide-answer>
            <option>It Depends</option>
            <option>1</option>
            <option>2</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * often can evaluate nodes in parallel
            * node vs. graph parallelization
            * simplified by message passing
            * each node has mutex, locked on message
            * internally use TBB task_group
              * describe TBB
          </aside>
        </section>
        <section>
          <h2>Task Stealing</h2>
          <img src="images/task_stealing_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * task/work stealing = work distribution strategy in TBB
            * analogy: 20 office workers, aka 20 threads
              * each has stack of paper with tasks
              * strategy 1: manager assigns tasks
              * strategy 2: idle workers walk around and steal stacks of tasks
            * thread task list, idle threads randomly steal
            * scales well to many threads
          </aside>
        </section>
        <section>
          <h2>Splitting the Work</h2>
          <img src="images/task_sizes_no_fonts.svg" />
          <p><em>Non-Uniform</em> and <em>Unknown</em> Task Sizes</p>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * problem: too many small nodes -> too many locks
            * ideal: use threading only when benefitial
            * usual solution: grain size
            * size < grain: no threading
            * problems: see slide
          </aside>
        </section>
        <section>
          <h2>Lazy Threading</h2>
          <img src="images/lazy_threading_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * solution: "lazy threading"
              * first single threaded
              * assume all tasks very small
              * tasks notify scheduler when large
              * scheduler becomes multi-threaded (+parent schedulers)
              * offloads remaining tasks
            * threadlocal scheduler callback
            * when to notify? reuse parallel_for grain size + few explicit
          </aside>
        </section>
        <section>
          <h2>Array Processing</h2>
          <img
            src="images/array_processing.png"
            style="width: 80%; margin-bottom: 0"
          />
          <hr />
          <h4>How many fields are evaluated here?</h4>
          <div
            class="polli-live-choice"
            id="how-many-field-evaluations"
            data-hide-answer
          >
            <option>1</option>
            <option>2</option>
            <option>3</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * end of lazy evaluation
            * next topic: evaluate functions on large data, usually in field evaluations
            * problem statement:
              * input for each index, compute output for each index
              * see slide example
            * goal: high performance, low peak memory
            * first: two things we are not doing yet
          </aside>
        </section>
        <section>
          <h2>No GPU Processing</h2>
          <img src="images/current_gpu_usage_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * no gpu, although well suited in general
              * part of scene evaluation, see slide, transfer cost
              * slow shader compilation (see eevee), lots of shaders
              * need fast cpu too
              * bottlenecks elsewhere (boolean, bvh tree, subdiv)
            * overall good goal, not a priority yet
          </aside>
        </section>
        <section>
          <h2>No Just-in-Time Compilation</h2>
          <img src="images/jit_pipeline_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * generate optimal machine code
            * Easier than gpu, similar problems (compile time!, fast fallback, other bottlenecks)
            * expected speedup? (in experiment < 2x, simple math, hard to be certain)
            * negilible speedup for more complex functions (sqrt, logarithm, noise)
            * many nodes needed before noticable speedup
            * not worth the complexity yet
          </aside>
        </section>
        <section>
          <h3><em>Latency</em>: Time per Element</h3>
          <h3><em>Throughput</em>: Elements per Time</h3>
          <hr />
          <h4>What should we optimize for?</h4>
          <div
            class="polli-live-choice"
            id="latency-vs-throughput"
            data-hide-answer
          >
            <option>Latency</option>
            <option>Throughput</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * choose primary optimization metric
            * contenders: latency, throughput
            * analogy: cycle path between two villages
              * single lane high quality: super fast, great latency, bad throughput
              * very wide path, bad quality: slow, bad latency, large throughput
            * simulate single particle fast vs. many particles fast
            * Solution: throughput
              * worsens latency, improves particles per second
            * general: one in terms of many vs. many in terms of one (optimizable)
            * latency important: drivers/constraints in armatures
          </aside>
        </section>
        <section>
          <h2>Multi Function</h2>
          <pre
            class="cpp"
          ><code data-trim data-line-numbers="2,3|4|6-8"><script type="text/template">
          // Simple multi-function that adds two integers.
          const VArray a = p.get_input_varray<int>("A");
          const VArray b = p.get_input_varray<int>("B");
          MutableSpan result = p.get_output_array<int>("Result");

          mask.foreach_index([&](const int i) {
            result[i] = a[i] + b[i];
          });
          </script></code></pre>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * similar to lazy-function: multi-function
            * process many elements
            * see slide
            * memory management in caller
            * inefficient: virtual functions, boilerplate
          </aside>
        </section>
        <section>
          <h2>Multi Function Build Utils</h2>
          <pre class="cpp"><code data-trim><script type="text/template">
          static auto fn = build::SI2_SO<int, int, int>(
            "Add",
            [](int a, int b) {
              return a + b;
            });
          </script></code></pre>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * reduce boilerplate with utils
            * devirtualization: automatically optimizes for different input types
            * `SI2_SO`
          </aside>
        </section>
        <section>
          <h2>Combinatorial Explosion</h2>
          <table style="font-size: 70%" class="combinatorial-explosion-table">
            <tr>
              <th>Case</th>
              <th>Input 1</th>
              <th>Input 2</th>
              <th>Input 3</th>
            </tr>
            <tr>
              <td>1</td>
              <td>Single</td>
              <td>Single</td>
              <td>Single</td>
            </tr>
            <tr>
              <td>2</td>
              <td>Single</td>
              <td>Single</td>
              <td>Array</td>
            </tr>
            <tr>
              <td>3</td>
              <td>Single</td>
              <td>Array</td>
              <td>Single</td>
            </tr>
            <tr>
              <td>4</td>
              <td>Single</td>
              <td>Array</td>
              <td>Array</td>
            </tr>
            <tr>
              <td>5</td>
              <td>Array</td>
              <td>Single</td>
              <td>Single</td>
            </tr>
            <tr>
              <td>6</td>
              <td>Array</td>
              <td>Single</td>
              <td>Array</td>
            </tr>
            <tr>
              <td>7</td>
              <td>Array</td>
              <td>Array</td>
              <td>Single</td>
            </tr>
            <tr>
              <td>8</td>
              <td>Array</td>
              <td>Array</td>
              <td>Array</td>
            </tr>
          </table>
          <p>
            <em>Binary Size</em> / <em>Compilation Time</em> vs.
            <em>Performance</em>
          </p>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * problem: inputs take many forms (single, array, other)
            * many input type combinations
            * trade-off, next slide details
          </aside>
        </section>
        <section>
          <h2>Controlling the Explosion</h2>
          <pre
            class="cpp"
          ><code data-trim data-line-numbers="6"><script type="text/template">
          static auto fn = build::SI2_SO<int, int, int>(
            "Add",
            [](int a, int b) {
              return a + b;
            },
            build::exec_presets::SomeSpanOrSingle<0>());
          </script></code></pre>
          <table style="font-size: 70%" class="devirtualization-table">
            <tr>
              <th>Method</th>
              <th style="text-align: center">
                <span style="font-style: italic">Map Range</span> Size
              </th>
              <th style="text-align: center; font-style: italic">Add</th>
              <th style="text-align: center; font-style: italic">Logarithm</th>
            </tr>
            <tr>
              <td><code>Simple</code></td>
              <td><code>-</code></td>
              <td><em>1296</em>&#8201;ms</td>
              <td><em>240</em>&#8201;ms</td>
            </tr>
            <tr>
              <td><code>Materialized</code></td>
              <td><em>+&#8201;30</em>&#8201;kb</td>
              <td><em>68</em>&#8201;ms</td>
              <td><em>71</em>&#8201;ms</td>
            </tr>
            <tr>
              <td><code>SomeSpanOrSingle</code></td>
              <td><em>+&#8201;137</em>&#8201;kb</td>
              <td><code>-</code></td>
              <td><code>-</code></td>
            </tr>
            <tr>
              <td><code>AllSpanOrSingle</code></td>
              <td><em>+&#8201;1100</em>&#8201;kb</td>
              <td><em>42</em>&#8201;ms</td>
              <td><em>63</em>&#8201;ms</td>
            </tr>
          </table>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * solution: 
              * high level control (easy experimentation)
              * good fallback (scales linearly with inputs)
            * template magic...
            * see slide
          </aside>
        </section>
        <section>
          <h2>Multi Threading</h2>
          <img src="images/multi_threading_grains_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Compared to multi-threading lazy function graph evaluation, it is much easier for array processing.
            * Array processing is embarrassingly parallel, so we can just split the work into chunks and evaluate them in parallel.
            * The nice side-benefit of working with chunks is that it reduces the peak memory usage.
              * That's because the chunk size also determines how large the arrays for intermediate results have to be.
            * The main question is which grain size should be used.
          </aside>
        </section>
        <section>
          <h2>Grain Size &#8211; Lower Bound</h2>
          <div id="grain-size-graph-1"></div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * There typically is not the perfect grain size as it depends on many parameters.
            * However, using some simple experiments one can make a good enough educated choice.
            * The graph shows how the grain size affects the evaluation time of a simple setup with a long chain of math nodes and lots of points (100M).
            * As one would expect, choosing a grain size that is too small is bad because it increases the overhead of distributing tasks to threads and the code has to iterate over all the nodes more often.
            * In this graph it looks like the grain size can't be too large, however that is far from true.
          </aside>
        </section>
        <section>
          <h2>Grain Size &#8211; Upper Bound</h2>
          <div id="grain-size-graph-2"></div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * The most common reason for why a grain size can be too large is that it decreases the number of threads that can work concurrently.
              * For example, if there are 100.000 elements and the grain size is 10.000, only 10 threads can work concurrently.
              * With a grain size of 50.000, only 2 threads can work at the same time.
            * However, that is not the cause for these huge performance drops we see in this graph.
              * I controlled for this by processing 100M elements, so that there is enough work for all threads even if the grain size is large.
            * Instead, what we see here is the effect of caching in the CPU.
              * At a grain size of 400k and 800k, some caches are full so that the evaluation has to work with the next slower cache level or main memory.
              * Note that typically we want to use cpu caches because they have a much lower latency than main memory.
              * However, here the latency actually does not matter. Instead, the memory bandwidth is the limiting factor.
              * Caches build into the CPU have bandwidths of about 1TB/s, while main memory is more in the range of 50GB/s.
              * Having all CPU cores read from main memory at the same time can easily saturate the memory bandwidth and even make it slower than if there were fewer running threads.
              * We also noticed that effect in other places, e.g. when copying large chunks of memory.
                * Using multiple-threads to copy memory can improve performance, but after a certain number of threads the performance drops again.
                * Internally, we limit the number of threads for such cases nowadays.
            * The good news is that we have a very long range of grain sizes that seem to deliver good performance.
              * Currently, we just picked the lower end of that range (10.000) as the grain size we use by default.
              * In more complex node trees, more intermediate buffers are necessary, but those will still fit in cache if the grain size is low enough.
            * Reusing memory for intermediate results is surprisingly important for the same reason a too large grain size can be bad.
            * Not reusing memory can increase peak memory usage quite a bit which makes the CPU caches less effective.
            * When an array that was just used before is used again, it's addresses are likely still in cache which improves memory access performance.
            * Obviously, just avoiding unnecessary calls into the global allocator is also beneficial.
            * Numbers on the slide show the performance of a simple node tree with absolutely no and with good memory reuse.
          </aside>
        </section>
        <section>
          <h2>SIMD</h2>
          <pre class="cpp"><code data-trim="">
          int i = 0;
          for (; i + 8 <= size; i += 8) {
            // Process chunks with SIMD and loop unrolling.
          }
          for (; i < size; i++) {
            // Process remaining elements.
          }
          </code></pre>
          <h4><span style="font-style: italic">Add</span> Node</h4>
          <table>
            <tr>
              <th>Mode</th>
              <th>Time</th>
              <th>Improvement</th>
            </tr>
            <tr>
              <td><code>Baseline</code></td>
              <td><em>72</em>&#8201;ms</td>
              <td style="text-align: center"><code>1.0</code>&#8201;x</td>
            </tr>
            <tr>
              <td><code>SSE2</code></td>
              <td><em>27</em>&#8201;ms</td>
              <td style="text-align: center"><code>2.7</code>&#8201;x</td>
            </tr>
            <tr>
              <td><code>AVX2</code></td>
              <td><em>23</em>&#8201;ms</td>
              <td style="text-align: center"><code>3.1</code>&#8201;x</td>
            </tr>
          </table>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * SIMD stands for Single Instruction, Multiple Data.
            * Here it refers to special instructions in modern CPUs that allow each core to process multiple elements at once.
            * E.g. instead of only performing a single integer addition, it can do 2, 4, or 8 or even 16 at the same time depending on the architecture.
            * Officially distributed Blender builds are only compiled with SSE2 support, which means that we are limited to processing 4 elements at once.
            * However, even with that we can get some nice speedups.
              * The numbers on the slide show the performance of some Add nodes without SIMD, with SSE2 and with AVX2 instructions.
              * As you can see, we get 2.6x better performance even with just SSE2 which is quite good considering that these numbers are measuring the full field evaluation and the optimization only affects a tiny part of it.
              * While we would benefit from larger SIMD registers, the performance gain would not be as great as one would hope without further changes.
              * To really benefit from those, we'd likely have to use JIT compilation to perform loop fusion to reduce the memory access overhead.
            * All vectorization we use for multi-functions is currently done the compiler and we just wrote the code carefully while making sure that the compiler can vectorize it.
              * Fortunately, all the simple math functions go through the same hot loop, so it only had to be optimized once.
          </aside>
        </section>
        <section>
          <h2>SIMD &#8211; Chunk Sizes</h2>
          <div id="simd-alignment-graph"></div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * When using SIMD, one usually has two loops.
              * One that is very fast and processes everything that can be processed with SIMD instructions.
              * However, if the number of elements is not divisible by the SIMD width times loop unrolling factor, a fallback loop is necessary that processes the remaining elements.
            * It's quite possible that more time is spend in the fallback loop than the main one of the number of elements is badly aligned.
            * The shown graph is from a performance experiment I did a few years ago.
              * It shows how performance gets worse the more elements there are that are not aligned.
            * When using standard `parallel_for` from TBB, the ranges are rarely well aligned.
            * We use a small wrapper around `parallel_for` for multi-functions, that ensure that the chunk sizes are better aligned.
          </aside>
        </section>
        <section>
          <h2>Final Thoughts</h2>
          <em>
            <p class="fragment custom blur">Optimize optimizability.</p>
            <p class="fragment custom blur">Don't be lazy with lazyness.</p>
            <p class="fragment custom blur">Estimate optimal performance.</p>
          </em>
        </section>
      </div>
    </div>
    <div
      id="my-progress"
      style="
        position: sticky;
        bottom: 0.4rem;
        left: 0;
        color: white;
        width: 100%;
        background-color: #00d6a3ff;
        border: 2px solid transparent;
        transform: scaleX(0);
        transform-origin: 0 0;
        transition: 0.8s cubic-bezier(0.26, 0.86, 0.44, 0.985);
      "
    ></div>
    <img
      id="my-progress-icon"
      src="images/socket.svg"
      style="
        position: sticky;
        bottom: 0;
        left: 0;
        transform-origin: 0 0;
        width: 1rem;
        transition: 0.8s cubic-bezier(0.26, 0.86, 0.44, 0.985);
        transform: translateX(0vw);
      "
    />

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>
      const standard_width = 960;
      const standard_height = 700;
      const graphs_container_ids = [];

      async function make_graph(
        source_file_path,
        container_id,
        x_name,
        y_name,
        filter_x_fn = (v) => true
      ) {
        const container_elem = document.getElementById(container_id);
        if (!container_elem) {
          console.error("Container not found:", container_id);
          return;
        }

        let csv_data = await d3.csv(source_file_path, d3.autoType);
        csv_data = csv_data.filter((v) => filter_x_fn(v[x_name]));
        graphs_container_ids.push(container_id);

        container_elem.style.width = "100%";
        container_elem.style.aspectRatio = "2 / 1";
        container_elem.style.display = "flex";
        container_elem.style.justifyContent = "center";
        container_elem.style.alignItems = "center";

        const trace1 = {
          x: csv_data.map((v) => v[x_name]),
          y: csv_data.map((v) => v[y_name]),
          type: "lines+markers",
          line: {
            color: "rgb(219, 166, 103)",
          },
        };

        const data = [trace1];

        const tick_font_size = 36;
        const title_font_size = 24;

        const layout = {
          paper_bgcolor: "transparent",
          plot_bgcolor: "transparent",
          showlegend: false,
          xaxis: {
            gridcolor: "#444",
            tickcolor: "transparent",
            tickformat: ",.0f",
            nticks: 5,
            tickfont: {
              color: "white",
              size: tick_font_size,
            },
            title: {
              text: x_name,
              font: {
                size: title_font_size,
                color: "white",
              },
              standoff: 20,
            },
          },
          yaxis: {
            gridcolor: "#444",
            tickcolor: "transparent",
            tickformat: ",.0f",
            nticks: 5,
            tickfont: {
              color: "white",
              size: tick_font_size,
            },
            title: {
              text: y_name,
              font: {
                size: title_font_size,
                color: "white",
              },
              standoff: 10,
            },
          },
          margin: {
            l: 110,
            r: 110,
            b: 100,
            t: 0,
            pad: 0,
          },
          scrollZoom: false,
          dragmode: false,
        };

        Plotly.newPlot(container_id, data, layout, {
          displayModeBar: false,
          responsive: false,
        });
      }

      make_graph(
        "analysis/grain_size_results.csv",
        "grain-size-graph-1",
        "Grain Size",
        "Time (s)",
        (x) => x < 20000
      );
      make_graph(
        "analysis/grain_size_results.csv",
        "grain-size-graph-2",
        "Grain Size",
        "Time (s)",
        (x) => x < 1000000
      );
      make_graph(
        "analysis/simd_alignment.csv",
        "simd-alignment-graph",
        "Array Size",
        "Performance",
        (x) => x >= 32 && x < 500
      );

      if (typeof polli_live !== "undefined") {
        polli_live.initialize({
          width: standard_width,
          height: standard_height,
        });
      }
      Reveal.initialize({
        hash: true,
        progress: false,
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
      });
      Reveal.addEventListener("slidechanged", function (event) {
        const currentSlide = event.currentSlide;
        if (!currentSlide) {
          return;
        }
        for (const container_id of graphs_container_ids) {
          const element = currentSlide.querySelector("#" + container_id);
          if (element) {
            Plotly.Plots.resize(element);
          }
        }
        const progress = Reveal.getProgress();
        const progress_bar_elem = document.getElementById("my-progress");
        const progress_icon_elem = document.getElementById("my-progress-icon");
        progress_bar_elem.style.transform = `scaleX(${progress})`;
        progress_icon_elem.style.transform = `translateX(${progress * 100}vw)`;
      });
    </script>
  </body>
</html>
