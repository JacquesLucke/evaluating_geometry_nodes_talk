<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />

    <title>Evaluating Geometry Nodes</title>

    <link rel="stylesheet" href="dist/reset.css" />
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/black.css" />

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/my_theme.css" />

    <script src="https://js.polli.live/main.js"></script>
    <!-- <script src="http://localhost:8080/main.js"></script> -->

    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/d3/7.9.0/d3.min.js"
      integrity="sha512-vc58qvvBdrDR4etbxMdlTt4GBQk1qjvyORR2nrsPsFPyrs+/u5c3+1Ct6upOgdZoIl7eq6k3a1UPDSNAQi/32A=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/plotly.js/2.35.0/plotly.min.js"
      integrity="sha512-L205fVN73b8Ft9dbuwTVGFb4FHVSOPpCLMQzcHa4r0+CuUswxzK/JRW7glZrpC+bO3Yaka0DYYDmtlhVbmnX+g=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <style>
      .reveal h1,
      .reveal h2,
      .reveal h3,
      .reveal h4,
      .reveal h5,
      .reveal h6 {
        text-transform: none;
      }

      hr {
        border-color: #444;
      }

      .reveal em {
        color: #dba667;
        font-style: normal;
      }

      .reveal pre code {
        max-height: 500px;
      }

      .reveal .progress {
        color: #00d6a3ff;
      }

      .fragment.blur {
        filter: blur(0.2em);
      }

      .fragment.blur.visible {
        filter: none;
      }

      .filter-lazy-function-graph-svg {
        filter: invert(90%);
      }
    </style>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2 class="r-fit-text">Evaluating Geometry Nodes</h2>
        </section>
        <section>
          <div
            style="display: grid; grid-template-columns: 2fr 3fr; height: 100%"
          >
            <div>
              <img
                class="polli-live-qr-code"
                style="
                  image-rendering: pixelated;
                  margin: 0;
                  width: 100%;
                  background-color: white;
                  border: 0.5em solid transparent;
                  border-radius: 0.5em;
                  box-sizing: border-box;
                "
              />
            </div>
            <div>
              Join the interactive part!
              <div
                class="polli-live-choice"
                id="join-interactivity"
                style="width: 100%"
              >
                <option>I'm in</option>
                <option>Wait</option>
              </div>
            </div>
          </div>
        </section>
        <section>
          <img src="images/topics_no_fonts.svg" style="width: 70%" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Start with showing some simpler evaluation methods to set the scene.
              * These are also methods that make sense when just starting out to build a new node system.
            * Then we have a look at how the geometry nodes graph is processed and how it is evaluated at a high level.
            * Lastly, we have a look at how math functions are evaluated as part of field evaluation.
            * This is a technical talk that's mainly geared towards developers curious about how geometry nodes works internally, or who are building their own evaluation system.
          </aside>
        </section>
        <section>
          <h2>What's your background?</h2>
          <div class="polli-live-continuum" id="what-is-your-background-poll">
            <option>Artistic</option>
            <option>Both</option>
            <option>Technical</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * I'm also curious about the background of the audience.
            * Personally, I'm more on the technical side, but I'm always most impressed by people who combine technical and artistic skills.
              * Like Simon Thommes who is a great help in the geometry nodes team bridging the gap between the two worlds.
            * I started building node systems for Blender about 10 years ago when I was still in school (Animation Nodes).
            * Hired by Blender after my studies in 2018 and am working remotely from the Berlin area nowadays.
            * Lead developer for geometry nodes since the beginning in 2020.
            * Best thing about the job is seeing all the cool stuff people make with the tools Hans and I implement in our living rooms, with the help of many others of course.
              * I'm thankful for everyone who shares their early experiments especially when testing new features. It's super helpful and motivating.
          </aside>
        </section>
        <section>
          <h2>Basics</h2>
          <img src="images/get_started.png" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * The image shows a simple node tree that I'll use as starting point to discuss some of the core concepts.
            * Using it I'll explain some simple graph evaluation strategies.
          </aside>
        </section>
        <section>
          <h2>Toposort</h2>
          <img src="images/toposort_no_fonts.svg" />
          <hr style="margin-top: 0" />
          <h4>Are there multiple possible orderings?</h4>
          <div class="polli-live-choice" id="unique-toposort" data-hide-answer>
            <option>Yes</option>
            <option>No</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * The simplest mechanims is to simply sort the nodes topologically.
              * That means that we order the nodes in a way that nodes always come after everything they depend on.
              * The ordering is always possible unless there are cycles in the node tree, but none of the other methods work in that case either.
            * Once sorted, we can just evaluate the nodes one after the other making sure we pass data correctly between them.
            * This general approach was used by Animation Nodes, with the addition that after sorting a Python script is generated that executes the nodes.
            * While very simple, this approach in its pure form has some significant limitations like:
              * All nodes are always evaluated, even if they are not needed.
              * It's hard to parallelize the evaluation.
            * While just using toposort is not great for evaluation, it's still a very important concept that's used in many places in Blender when analysing node trees.
          </aside>
        </section>
        <section>
          <h2>Two-Pass Evaluation</h2>
          <img src="images/two_pass.png" />
          <img src="images/two_pass_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * A way to remedy the first problem is to process nodes in two passes.
              * In the first pass, we figure out which nodes are needed for the final result.
              * In the second pass, we only evaluate the nodes that are needed.
            * In the example here that means that we detect that the second Add node is not needed and it can be ignored.
            * In this case it's obvious because the output of the node is not used at all.
            * However, some more complex cases can be handled with this approach too.
              * The dependency graph evaluation uses a separate pass to first determine when objects are visible in the case of animated visibility.
              * Cycles uses this approach to optimize e.g. Mix nodes when the mix factor is a constant 0 or 1 so that one of the inputs can be ignored.
          </aside>
        </section>
        <section>
          <h2>Request Based Evaluation</h2>
          <img src="images/request_based_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * So far we've only looked at what I'd call push based evaluation.
              * That means that we first determine all nodes to evaluate and then start evaluating them at the beginning.
            * That can be good enough for many kinds of node systems.
              * It's used by shader (eevee and cycles) and compositor evaluation.
            * It's far from ideal for geometry nodes:
              * The problem is that we just can't determine which nodes have to be evaluated before having evaluated potentially large parts of the node tree already.
              * Also, the condition of switch nodes may depend on the result of other switch nodes.
              * Users have to be able to trust that when a switch node disables part of the node tree, that those nodes really won't be evaluated.
            * A solution is to use what I call a pull based evaluation system.
              * No preprocessing is necessary to determine which nodes are necessary.
              * That means, the evaluation does not start at the input, but at the output.
              * When a node is evaluated, it first requests the missing inputs from the nodes that come before.
              * Importantly, it does not have to request all of them at the same time.
                * For example, the switch node first only requests the condition input.
                * Once that comes back, it can request one of the remaining inputs.
            * This can be implemented relatively easily with a recursive evaluator.
              * Was done for the first geometry nodes evaluator.
              * Has significant limitations:
                * Single threaded
                * Long chains of nodes result in a long call stack which can lead to a stack overflow with just a few hundred nodes.
            * Solving this generically comes with some extra complexity, especially if multi-threaded evaluation has to be supported too.
            * Next I'll present the internal evaluation system we actually use for geometry nodes.
          </aside>
        </section>
        <section>
          <h2>Lazy Function</h2>
          <pre
            class="cpp"
          ><code data-trim data-line-numbers="2|3|4,5,11|6,7,12,13|8,14"><script type="text/template">
          // Simple lazy switch node.
          bool *condition = p.get_or_request("Condition");
          if (!condition) return;
          if (*condition) {
            p.set_input_unused("False");
            Geometry *value = p.get_or_request("True");
            if (!value) return;
            p.set_output("Value", *value);
          }
          else {
            p.set_input_unused("True");
            Geometry *value = p.get_or_request("False");
            if (!value) return;
            p.set_output("Value", *value);
          }
          </script></code></pre>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * At the core of our lazy evaluation system is the `LazyFunction` class.
            * All nodes are implemented as lazy-function in Geometry Nodes.
              * That may not be immediately obvious when working on nodes though.
              * That's because most nodes use a more high-level API that hides away the lazyness aspect.
              * More special nodes like the different kinds of Switch nodes are implemented directly as `LazyFunction` though.
            * A lazy function uses the following methods to interact with its inputs and outputs...
            * The lazy-function API is independent of geometry nodes, so it can also be tested independently.
          </aside>
        </section>
        <section>
          <h2>Lazy Function Graph</h2>
          <img
            src="images/simple_lazy_function_graph.png"
            style="width: 80%; margin-bottom: 0px"
          />
          <img
            src="images/simple_lazy_function_graph.svg"
            style="width: 90%; margin-top: 0"
          />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Each geometry node tree is first converted into a lazy function graph like the one shown here.
            * It's typically very similar to the corresponding node tree with some differences:
              * It has some extra stuff that's needed to handle the lifetimes of anonymous attributes.
                * See the "Usage" and "Propagate" sockets.
              * Zones are also represented differently in the graph.
            * The geometry nodes modifier or node tools operator then takes this generated lazy-function graph and evaluates it.
            * For that it provides all the inputs and requests all the main outputs.
            * It's a limitation that the modifier always has to provide all inputs, but lazyness is not possible outside of the modifier yet.
            * For debugging purposes, this graph can be visualized using a Core Debug Tools add-on that can be installed via the extensions platform.
          </aside>
        </section>
        <section>
          <h2>Scheduling &#8211; Message Priority</h2>
          <img src="images/shared_mesh_references_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * The order in which the messages are processed it not important for correctness.
            * However, it can have a significant impact on performance.
            * In this case we'd expect that we never have to make a copy of the cube mesh.
            * A copy could happen though if the condition is True.
              * In that case, the cube mesh is forwarded to the Set Position node and Switch node.
              * Internally there are now two references to the cube mesh.
              * So when the Set Position node wants to modify it, it first has to make a copy to avoid changing the other one.
            * This is solved by giving the messages that specify that a certain input is not required higher priority.
              * Then we know that the False input of the switch node is not needed and the Set Position node is the sole owner of the mesh.
            * This used to be a relatively large problem, but it's a bit better now since we make extensive use of implicit sharing on geometry data structures.
              * That means that when a geometry is copied, we rarely have to copy large arrays, but only increase their references counts.
          </aside>
        </section>
        <section>
          <h3>Scheduling &#8211; Breadth or Depth First?</h3>
          <img src="images/breadth_vs_depth_first.png" />
          <div
            class="polli-live-choice"
            id="breadth-vs-depth-first"
            data-hide-answer
          >
            <option>Breadth First</option>
            <option>Depth First</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Messages may schedule nodes. Now the question is which node to evaluate next when multiple nodes are scheduled.
            * There are two main strategies:
              * In breadth first scheduling, we evaluate nodes in the order in which they got scheduled.
                * So after the Join Geometry node is scheduled, all three Bounding Box nodes are evaluated, which then schedule the three primitive nodes.
                * Only after all three primitive nodes have been executed, the bounding box nodes are evaluated.
              * In depth first scheduling, we always evaluate the node that was last scheduled next.
                * So after the first bounding box node, the corresponding primitive node is evaluted which schedules the bounding box node again.
                * Then the bounding box node is evaluated before the next node is run.
            * What do you think is the better main strategy?
            * Depth first turns out to work better usually, because it leads to a lower peak memory usage.
            * Also it makes it more likely that the next evaluated node works on the same data that the previous node just created.
              * This improves cache utilization.
          </aside>
        </section>
        <section>
          <h2>Lazy Function Composition</h2>
          <p>
            <code><em>LazyFunction</em></code> &#10230;
            <code><em>Graph</em></code> &#10230;
            <code><em>LazyFunction</em></code>
          </p>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * We have a already seen that multiple lazy functions can be combined in a graph.
            * An important and powerful design aspect is that it is possible to wrap the entire graph in a new lazy function.
            * This new function can then be used as a single node in a new graph and behaves pretty much exactly the same as if all the nodes were inlined.
            * This is quite powerful, because it allows building many smaller graphs which can be combined in a bigger one without loosing any of the lazyness.
            * We make extensive use of that for node groups and zones.
          </aside>
        </section>
        <section>
          <h2>Node Groups</h2>
          <img
            src="images/node_group.svg"
            class="filter-lazy-function-graph-svg"
          />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * There are two main ways to handle node groups during node evaluation.
            * One can inline all nodes into a single node tree or one can keep them separate.
            * Always inlining has the benefit that it can be done in a preprocessing step and the code afterwards does not have to worry about it anymore.
              * Specifically, the lazy function graph evaluator would not have to be able to handle nested functions.
              * Furthermore, some optimizations may be easier to implement on a flat graph instead of a nested one.
            * All node systems except geometry nodes flatten the entire node tree before evaluation currently.
            * Geometry nodes does not inline node groups because:
              * The evaluator would have to be able to handle nesting anyway because of zones.
              * Geometry nodes trees tend to get much bigger than in other node systems.
                * Having more than ten thousand nodes after everything is inlined is not uncommon.
                * It's quite expensive to inline all of that.
                * Especially, considering that often only a much smaller part is actually evaluated and the rest is disabled by switch nodes when using high level assets.
              * Currently, we don't have the need to implement optimizations that would require inlining, but if that's really necessary, we can still selectively inline some node groups.
          </aside>
        </section>
        <section>
          <h2>Zones</h2>
          <img src="images/repeat_zone.png" style="margin-bottom: -40px" />
          <img
            src="images/repeat_zone.svg"
            class="filter-lazy-function-graph-svg"
          />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * When the geometry nodes tree is converted into a lazy-function graph, each zone becomes a single node.
            * A separate graph is created for each zone.
            * What kind of graph is created depends on the type of zone.
            * The Repeat and For Each zone dynamically create a graph that based on the number of iterations.
            * The shown graph is for a repeat zone with 3 iterations. Note how there are 3 instances of the Repeat Body node which is yet another graph that contains all nodes inside the zone.
            * This approach has the significant advantage that the existing graph evaluator can be used to evaluate these zones without any extra complexity.
            * Using the existing evaluator also means that we get all the scheduling and multi-threading optimizations of it for free.
              * For example, it's perfectly possible that nodes in multiple repeat iterations are evaluated at the same time.
            * This approach is not ideal for cases where the work done in the zone body is very cheap.
              * In the future we could try to detect such cases and evaluate such zones in a different way that is faster but does not support lazyness to the same degree.
          </aside>
        </section>
        <section>
          <h2>Scheduling &#8211; Multi Threading</h2>
          <img
            src="images/multi_threading_simple.png"
            style="margin-bottom: 0; width: 80%"
          />
          <hr style="margin-top: -10px; margin-bottom: 10" />
          <h4>How many threads should be used here?</h4>
          <div class="polli-live-choice" id="how-many-threads" data-hide-answer>
            <option>It Depends</option>
            <option>1</option>
            <option>2</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * In many cases it is possible to evaluate nodes in parallel.
            * The message passing architecture of the evaluator makes it fairly straight to implement basic multi-threading.
            * Nodes that are scheduled can be evaluated in parallel.
            * Each node has a mutex that can be locked when sending a message to it.
            * Internally that uses a task group provided by the TBB library.
          </aside>
        </section>
        <section>
          <h2>Task Stealing</h2>
          <img src="images/task_stealing_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Task stealing is the strategy used to distribute work between threads in the TBB library.
            * It roughly works like so:
              * Each thread has its own queue of tasks to execute.
              * When a thread runs out of tasks, it randomly picks another thread and steals half of its tasks.
              * After a short while, each thread will have work assuming there is enough work.
            * This is different from e.g. explicitly assigning tasks to threads and has some nice performance and scaling benefits.
          </aside>
        </section>
        <section>
          <h2>Splitting the Work</h2>
          <img src="images/task_sizes_no_fonts.svg" />
          <p><em>Non-Uniform</em> and <em>Unknown</em> Task Sizes</p>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * There is one big issue with the approach I described so far and that is that each communication across threads and each lock comes at a cost.
            * This cost can be much higher than the actual work done by the nodes, especially when there are many cheap nodes like math nodes.
            * Ideally, we'd like to detect situations when multi-threading is beneficial and only then use it.
            * This is not trivial, but lets first look at how that's done in simpler cases.
            * We always specify the so called "grain size" which is the size of a unit of work.
            * If the total number of elements to process is smaller, all work is done on the current thread without any multi-threading overhead.
            * Unfortunately, the same approach does not work for graph evaluation because of two main reasons:
              * The tasks do not have a uniform size. That implies that a simple grain size does not work because that depends on the size.
              * Furthermore, we do not even know the task sizes in advance. Most nodes in geometry nodes can be cheap or expensive depending on the specific inputs.
          </aside>
        </section>
        <section>
          <h2>Lazy Threading</h2>
          <img src="images/lazy_threading_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * The solution is to use an approach I call "lazy threading". That works as follows:
              * By default, the evaluator is single threaded, so no communication between threads and locks are necessary.
              * While a task is being executed, it may inform the scheduler that invoked it that it takes a while.
              * The scheduler can then decide to enable multi-threading and to move all tasks that are already scheduled to a task group that other threads can steal from.
              * After multi-threading is enabled in the scheduler, all messages that are passed around need locks.
              * However, note that all the nested node graphs are still evaluated in single-thread mode which typically contain the majority of nodes.
            * The main remaining question is when and how tasks send messages to the scheduler.
              * It works by letting the scheduler put a custom callback into thread-local storage that the tasks can call (with a simple API).
              * The when is more tricky because it would be annoying to add extra scheduler specific code in lots of places.
              * Fortunately, I noticed that we do have that code in many places already.
              * All the grain sizes we use for `parallel_for` already provide heuristics that can be reused here by just adding a little bit of extra code in `parallel_for`.
          </aside>
        </section>
        <section>
          <h2>Array Processing</h2>
          <img
            src="images/array_processing.png"
            style="width: 80%; margin-bottom: 0"
          />
          <hr />
          <h4>How many field evaluations happen here?</h4>
          <div
            class="polli-live-choice"
            id="how-many-field-evaluations"
            data-hide-answer
          >
            <option>1</option>
            <option>2</option>
            <option>3</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * This concludes the lazy evaluation section of the talk.
            * Next, I'll talk about how we process large quantities of data in geometry nodes.
            * Those are typically field evaluations potentially involving attributes.
            * The basic problem statement is fairly simple:
              * We have an input for each index and want to compute an output for each index.
              * In this example, we have the radius attribute and index field as input and two math nodes that should be processed.
            * The goal is of course to do that efficiently and with low peak memory usage.
            * Before talking about the actual implementation, I want to mention two approaches that we do not use yet.
          </aside>
        </section>
        <section>
          <h2>No GPU Processing</h2>
          <img src="images/current_gpu_usage_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Geometry Nodes does not use any GPU acceleration yet.
            * That is even though all this array processing would be very suitable for the GPU.
            * The main reasons are:
              * Using the GPU from Geometry Nodes is more complex than using it in the places that use it already like rendering.
                * Most systems that use the GPU in Blender run at the very end after the entire scene has been processed. Its output often does not have to be copied back to main memory.
                * However, geometry nodes runs in the middle of the dependency graph evaluation and its output is frequently used by other parts of Blender that can't run on the GPU.
                * Moving data back and forth between the CPU and GPU is expensive and detecting when it is worth it is not trivial.
                * Having many threads that concurrently evaluate geometry nodes interact with the GPU might also need cause problems that we have to develop solutions for.
              * Shader compilation can be slow and it's not acceptable to have to wait for it all the time.
                * Evaluating geometry nodes would potentially require lots of shaders to be compiled.
              * In typical scenes, the array processing is rarely the bottleneck given the optimizations I'll talk about.
                * Very common bottlenecks are mesh boolean, bvh tree lookups and subdivision surfaces.
              * We need an efficient CPU implementation anyway.
            * That said, there are certainly use-cases that would benefit from better GPU support and it's something we'll hopefully look into more in the future.
          </aside>
        </section>
        <section>
          <h2>No Just-in-Time Compilation</h2>
          <img src="images/jit_pipeline_no_fonts.svg" />
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Using LLVM it's fairly straight forward to generate highly optimized machine code at runtime nowadays.
            * The motivation for using it is that it can generate code that does the array processing much faster than without code generation.
            * However, that also comes at a cost:
              * Similar to shader compilation, compiling with llvm can be quite slow introducing wait times that I just don't want to have in geometry nodes.
              * It does add a fair amount of complexity and makes debugging harder.
              * We still need an efficient fallback implementation anyway.
            * Assuming we can hide the wait times by using the fallback implementation during compilation, all that complexity is only worth it if it actually makes things significantly faster.
              * I tried to quantify how much speedup we can expect a couple of times and it was never all that much.
              * In the best case, I maybe got a 2x speedup over what we have a now, but that is assuming you have lots of very simple operations and by that I mean just addition, subtraction and multiplication.
                * Any slightly more complex operation like division or square root can easily make the difference negilible. Not to mention something like noise functions which are orders of magnitudes slower.
                * One also already needs lots of math nodes before the benefit becomes noticable, because even just filling a large array with the results is slow compared to the actual computation.
            * So, there are situations that can benefit from JIT compilation, but it's not enough to justify prioritizing it over many other topics.
          </aside>
        </section>
        <section>
          <h3><em>Latency</em>: Time per Element</h3>
          <h3><em>Throughput</em>: Elements per Time</h3>
          <hr />
          <h4>Which should we optimize for?</h4>
          <div
            class="polli-live-choice"
            id="latency-vs-throughput"
            data-hide-answer
          >
            <option>Latency</option>
            <option>Throughput</option>
          </div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * With that out of the way, we can talk about the actual implementation.
            * When designing a system like this, it's important to know what you're optimizing exactly.
            * There are two main time metrics that can be optimized:
              * Latency is the time it takes to evaluate a single element. For example, when I have two noise functions and one takes 100ns to evaluate for one position and the other 200ns, the first would have better latency.
              * Throughput is the number of elements that can be evaluated per time. In the same example, if the first noise function can compute 10.000 positions per ms and the second one 20.000, then the second has a better throughput.
            * Importantly, optimizing one of them typically implies making the other metric worse.
            * What do you think which metric we should optimize for?
            * We optimize for throughput, because in geometry nodes it typically does not matter how long a single operation takes.
              * Instead it matters that we can do as many as possible per second.
            * There are cases where optimizing for latency is important though, e.g. when evaluating all the dependencies in an armature or drivers in general.
          </aside>
        </section>
        <section>
          <h2>Multi Function</h2>
          <pre
            class="cpp"
          ><code data-trim data-line-numbers="2,3|4|6-8"><script type="text/template">
          // Simple multi-function that adds two integers.
          const VArray a = p.get_input_varray<int>("A");
          const VArray b = p.get_input_varray<int>("B");
          MutableSpan result = p.get_output_array<int>("Result");

          mask.foreach_index([&](const int i) {
            result[i] = a[i] + b[i];
          });
          </script></code></pre>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Similar to lazy functions from before, we also have a multi-function API.
            * The name comes from the fact that these functions are designed to be evaluated on multiple elements at the same time.
            * They handle inputs and outputs different from lazy-functions using the shown functions.
            * Internally, we use slightly different names but with the same general semantics.
            * `get_input_varray` returns a so called "virtual array".
              * That is a readonly array-like data structure that is used to access the input values.
              * Often it is either backed by an actual array or just a single value. In some more rare cases it can also have a custom callback, e.g. for the index field.
              * Converting everything to arrays is prohibilitively expensive, because usually most inputs are single values.
            * `get_output_array` returns a writable array (not a virtual one) that the multi-function can write the results into.
            * This design puts the burden of managing input and outputs arrays on the caller which is makes it easier to reuse arrays that are already allocated.
            * This code implements a simple function that adds two integers.
            * In it's current state, it's not very efficient.
              * That's because there is virtual function call overhead every time an input is accessed.
              * For some kinds of functions that's fine, but here that overhead is more costly than the operation itself.
              * It's possible to check if a virtual array contains an actual array or single value and to optimize for these cases.
              * That adds a fair amount of boilerplate though.
              * While it's possible to generalize this "devirtualization", it's still annoying to have to do for every math function.
          </aside>
        </section>
        <section>
          <h2>Multi Function Build Utils</h2>
          <pre class="cpp"><code data-trim><script type="text/template">
          static auto fn = build::SI2_SO<int, int, int>(
            "Add",
            [](int a, int b) {
              return a + b;
            });
          </script></code></pre>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * For that reason, we have simple utilities that build an entire multi-function from a lambda.
            * Internally, that can implement optimizations like devirtualization.
            * The `SI2_SO` name specifies that the function takes two inputs and one output.
          </aside>
        </section>
        <section>
          <h2>Combinatorial Explosion</h2>
          <p>
            <em>Binary Size</em> /
            <em>Compilation Time</em>
          </p>
          <p>vs.</p>
          <p><em>Performance</em></p>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * There is one big problem with generating optimized versions for the different kinds of inputs to a multi-function.
            * If each input can be a single value, or array or something else, we need 3^#Inputs different versions.
            * For functions with two inputs, that can be ok, but it becomes a real problem when the number of inputs increases.
          </aside>
        </section>
        <section>
          <h2>Controlling the Explosion</h2>
          <pre
            class="cpp"
          ><code data-trim data-line-numbers="6"><script type="text/template">
          static auto fn = build::SI2_SO<int, int, int>(
            "Add",
            [](int a, int b) {
              return a + b;
            },
            build::exec_presets::AllSpanOrSingle());
          </script></code></pre>
          <p class="fragment custom blur">
            <code>Add:</code> <em>1296</em>&#8201;ms &#10230;
            <em>68</em>&#8201;ms &#10230; <em>42</em>&#8201;ms
          </p>
          <p class="fragment custom blur">
            <code>Logarithm:</code>
            <em>240</em>&#8201;ms &#10230; <em>71</em>&#8201;ms &#10230;
            <em>63</em>&#8201;ms
          </p>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * We deal with that problem in two ways.
            * First, we have the ability to control which cases are generated at a high level with a single line.
              * This makes it easy to check the performance impact of generating more or less cases.
            * Second, we have a good fallback path that handles most cases very well with a single code path.
            * The numbers show the time it takes to evaluate many math nodes on 10 million points.
              * It takes over a second when there is the virtual function call overhead for each access.
              * Our optimized fallback implementation only needs 68ms.
              * The fully optimized version is even faster at 42ms but this produces quite a bit more code.
            * The more complex the functions get, the closer these results are.
          </aside>
        </section>
        <section>
          <h2>Multi Threading</h2>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Compared to multi-threading lazy function graph evaluation, it is much easier for array processing.
            * Array processing is embarrassingly parallel, so we can just split the work into chunks and evaluate them in parallel.
            * The nice side-benefit of working with chunks is that it reduces the peak memory usage.
              * That's because the chunk size also determines how large the arrays for intermediate results have to be.
            * The main question is which grain size should be used.
          </aside>
        </section>
        <section>
          <h2>Grain Size &#8211; Lower Bound</h2>
          <div id="grain-size-graph-1"></div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * There typically is not the perfect grain size as it depends on many parameters.
            * However, using some simple experiments one can make a good enough educated choice.
            * The graph shows how the grain size affects the evaluation time of a simple setup with a long chain of math nodes and lots of points (100M).
            * As one would expect, choosing a grain size that is too small is bad because it increases the overhead of distributing tasks to threads and the code has to iterate over all the nodes more often.
            * In this graph it looks like the grain size can't be too large, however that is far from true.
          </aside>
        </section>
        <section>
          <h2>Grain Size &#8211; Upper Bound</h2>
          <div id="grain-size-graph-2"></div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * The most common reason for why a grain size can be too large is that it decreases the number of threads that can work concurrently.
              * For example, if there are 100.000 elements and the grain size is 10.000, only 10 threads can work concurrently.
              * With a grain size of 50.000, only 2 threads can work at the same time.
            * However, that is not the cause for these huge performance drops we see in this graph.
              * I controlled for this by processing 100M elements, so that there is enough work for all threads even if the grain size is large.
            * Instead, what we see here is the effect of caching in the CPU.
              * At a grain size of 400k and 800k, some caches are full so that the evaluation has to work with the next slower cache level or main memory.
              * Note that typically we want to use cpu caches because they have a much lower latency than main memory.
              * However, here the latency actually does not matter. Instead, the memory bandwidth is the limiting factor.
              * Caches build into the CPU have bandwidths of about 1TB/s, while main memory is more in the range of 50GB/s.
              * Having all CPU cores read from main memory at the same time can easily saturate the memory bandwidth and even make it slower than if there were fewer running threads.
              * We also noticed that effect in other places, e.g. when copying large chunks of memory.
                * Using multiple-threads to copy memory can improve performance, but after a certain number of threads the performance drops again.
                * Internally, we limit the number of threads for such cases nowadays.
            * The good news is that we have a very long range of grain sizes that seem to deliver good performance.
              * Currently, we just picked the lower end of that range (10.000) as the grain size we use by default.
              * In more complex node trees, more intermediate buffers are necessary, but those will still fit in cache if the grain size is low enough.
          </aside>
        </section>
        <section>
          <h2>Memory Reuse</h2>
          <p><em>2700</em>&#8201;ms &#10230; <em>245</em>&#8201;ms</p>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * Reusing memory for intermediate results is surprisingly important for the same reason a too large grain size can be bad.
            * Not reusing memory can increase peak memory usage quite a bit which makes the CPU caches less effective.
            * When an array that was just used before is used again, it's addresses are likely still in cache which improves memory access performance.
            * Obviously, just avoiding unnecessary calls into the global allocator is also beneficial.
            * Numbers on the slide show the performance of a simple node tree with absolutely no and with good memory reuse.
          </aside>
        </section>
        <section>
          <h2>SIMD</h2>
          <p>
            <em>72</em>&#8201;ms &#10230; <em>27</em>&#8201;ms &#10230;
            <em>23</em>&#8201;ms
          </p>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * SIMD stands for Single Instruction, Multiple Data.
            * Here it refers to special instructions in modern CPUs that allow each core to process multiple elements at once.
            * E.g. instead of only performing a single integer addition, it can do 2, 4, or 8 or even 16 at the same time depending on the architecture.
            * Officially distributed Blender builds are only compiled with SSE2 support, which means that we are limited to processing 4 elements at once.
            * However, even with that we can get some nice speedups.
              * The numbers on the slide show the performance of some Add nodes without SIMD, with SSE2 and with AVX2 instructions.
              * As you can see, we get 2.6x better performance even with just SSE2 which is quite good considering that these numbers are measuring the full field evaluation and the optimization only affects a tiny part of it.
              * While we would benefit from larger SIMD registers, the performance gain would not be as great as one would hope without further changes.
              * To really benefit from those, we'd likely have to use JIT compilation to perform loop fusion to reduce the memory access overhead.
            * All vectorization we use for multi-functions is currently done the compiler and we just wrote the code carefully while making sure that the compiler can vectorize it.
              * Fortunately, all the simple math functions go through the same hot loop, so it only had to be optimized once.
          </aside>
        </section>
        <section>
          <h2>SIMD &#8211; Chunk Sizes</h2>
          <div id="simd-alignment-graph"></div>
          <!-- prettier-ignore -->
          <aside class="notes" data-markdown>
            * When using SIMD, one usually has two loops.
              * One that is very fast and processes everything that can be processed with SIMD instructions.
              * However, if the number of elements is not divisible by the SIMD width times loop unrolling factor, a fallback loop is necessary that processes the remaining elements.
            * It's quite possible that more time is spend in the fallback loop than the main one of the number of elements is badly aligned.
            * The shown graph is from a performance experiment I did a few years ago.
              * It shows how performance gets worse the more elements there are that are not aligned.
            * When using standard `parallel_for` from TBB, the ranges are rarely well aligned.
            * We use a small wrapper around `parallel_for` for multi-functions, that ensure that the chunk sizes are better aligned.
          </aside>
        </section>
        <section>
          <h2>Key Takeaways</h2>
          <em>
            <p>Optimize optimizability.</p>
            <p>Don't be lazy with lazyness.</p>
            <p>Compare against optimal performance.</p>
          </em>
        </section>
      </div>
    </div>
    <div
      id="my-progress"
      style="
        position: sticky;
        bottom: 0.4rem;
        left: 0;
        color: white;
        width: 100%;
        background-color: #00d6a3ff;
        border: 2px solid transparent;
        transform: scaleX(0);
        transform-origin: 0 0;
        transition: 0.8s cubic-bezier(0.26, 0.86, 0.44, 0.985);
      "
    ></div>
    <img
      id="my-progress-icon"
      src="images/socket.svg"
      style="
        position: sticky;
        bottom: 0;
        left: 0;
        transform-origin: 0 0;
        width:1rem;
        transition: 0.8s cubic-bezier(0.26, 0.86, 0.44, 0.985);
        transform: translateX(0vw);
      "
    ></div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>
      const standard_width = 960;
      const standard_height = 700;
      const graphs_container_ids = [];

      async function make_graph(
        source_file_path,
        container_id,
        x_name,
        y_name,
        filter_x_fn = (v) => true
      ) {
        const container_elem = document.getElementById(container_id);
        if (!container_elem) {
          console.error("Container not found:", container_id);
          return;
        }

        let csv_data = await d3.csv(source_file_path, d3.autoType);
        csv_data = csv_data.filter((v) => filter_x_fn(v[x_name]));
        graphs_container_ids.push(container_id);

        container_elem.style.width = "100%";
        container_elem.style.aspectRatio = "2 / 1";
        container_elem.style.display = "flex";
        container_elem.style.justifyContent = "center";
        container_elem.style.alignItems = "center";

        const trace1 = {
          x: csv_data.map((v) => v[x_name]),
          y: csv_data.map((v) => v[y_name]),
          type: "lines+markers",
          line: {
            color: "rgb(219, 166, 103)",
          },
        };

        const data = [trace1];

        const tick_font_size = 36;
        const title_font_size = 24;

        const layout = {
          paper_bgcolor: "transparent",
          plot_bgcolor: "transparent",
          showlegend: false,
          xaxis: {
            gridcolor: "#444",
            tickcolor: "transparent",
            tickformat: ",.0f",
            nticks: 5,
            tickfont: {
              color: "white",
              size: tick_font_size,
            },
            title: {
              text: x_name,
              font: {
                size: title_font_size,
                color: "white",
              },
              standoff: 20,
            },
          },
          yaxis: {
            gridcolor: "#444",
            tickcolor: "transparent",
            tickformat: ",.0f",
            nticks: 5,
            tickfont: {
              color: "white",
              size: tick_font_size,
            },
            title: {
              text: y_name,
              font: {
                size: title_font_size,
                color: "white",
              },
              standoff: 10,
            },
          },
          margin: {
            l: 110,
            r: 110,
            b: 100,
            t: 0,
            pad: 0,
          },
          scrollZoom: false,
          dragmode: false,
        };

        Plotly.newPlot(container_id, data, layout, {
          displayModeBar: false,
          responsive: false,
        });
      }

      make_graph(
        "analysis/grain_size_results.csv",
        "grain-size-graph-1",
        "Grain Size",
        "Time (s)",
        (x) => x < 20000
      );
      make_graph(
        "analysis/grain_size_results.csv",
        "grain-size-graph-2",
        "Grain Size",
        "Time (s)",
        (x) => x < 1000000
      );
      make_graph(
        "analysis/simd_alignment.csv",
        "simd-alignment-graph",
        "Elements",
        "Iterations per ns",
        (x) => x >= 32 && x < 500
      );

      polli_live.initialize({
        width: standard_width,
        height: standard_height,
      });
      Reveal.initialize({
        hash: true,
        progress: false,
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
      });
      Reveal.addEventListener("slidechanged", function (event) {
        const currentSlide = event.currentSlide;
        if (!currentSlide) {
          return;
        }
        for (const container_id of graphs_container_ids) {
          const element = currentSlide.querySelector("#" + container_id);
          if (element) {
            Plotly.Plots.resize(element);
          }
        }
        const progress = Reveal.getProgress();
        const progress_bar_elem = document.getElementById("my-progress");
        const progress_icon_elem = document.getElementById("my-progress-icon");
        progress_bar_elem.style.transform = `scaleX(${progress})`;
        progress_icon_elem.style.transform = `translateX(${progress * 100}vw)`;
      });
    </script>
  </body>
</html>
